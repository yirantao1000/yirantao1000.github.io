<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yiran Tao</title>
  
  <meta name="author" content="Yiran Tao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-6EEWRYJJGY"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());
	  gtag('config', 'G-6EEWRYJJGY');
  </script>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yiran Tao</name>
              </p>
              <p>Hi there! I am a second year MS in Robotics (MSR) student at <a href="https://www.ri.cmu.edu/">the Robotics Institute</a>, <a href="https://www.cs.cmu.edu/">Carnegie Mellon University</a>. I am working with Prof. <a href="https://zackory.com/">Zackory Erickson</a> in <a href="https://rchi-lab.github.io/">Robotic Caregiving and Human Interaction Lab</a>. I'm fascinated by the potential of robots to collaborate with humans, which is why my current research focuses on developing robot learning algorithms for Human-Robot Interaction (HRI). 
	      </p>  
		<p>I obtained my bachelor's degree from <a href="https://www.whu.edu.cn/">Wuhan University</a>, China, where I am fortunate to be advised by Prof. <a href="http://iip.whu.edu.cn/">Zhenzhong Chen</a> to work on computer vision research. I also spent half a year at <a href="https://www.harvard.edu/">Harvard University</a> as well as <a href="https://www.mit.edu/">MIT</a> as a visiting undergraduate student, and was fortunate to be advised by Prof. <a href="https://vcg.seas.harvard.edu/people/hanspeter-pfister">Hanspeter Pfister</a> to work on biomedical image analysis. 
	      </p>  
              
              
              <p style="text-align:center">
                <a href="mailto:yirantao1000@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/YiranTao_CV.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp/&nbsp -->
                <a href="https://www.linkedin.com/in/yiran-tao-7957a6188/">Linkedin</a> &nbsp/&nbsp
                <a href="https://github.com/yirantao1000">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/image0.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/image0.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I have primarily focused on robot learning, human-robot interaction (HRI), and computer vision. I am also broadly interested in the applications of machine learning across various academic fields.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/lams.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://arxiv.org/abs/1503.00848" id="MCG_journal"> -->
              <a href="https://lams-assistance.github.io/">
<!--               <a> -->
                <papertitle>LAMS: LLM-Driven Automatic Mode Switching for Assistive Teleoperation</papertitle>
              </a>
              <br>
              <strong>Yiran Tao</strong>*, Jehan Yang*, Dan Ding, Zackory Erickson
              <br>
              <em>HRI 2025 (<strong>Best Paper Finalist</strong>)</em>
              <br>            
              <p>We introduce LLM-Driven Automatic Mode Switching (LAMS), a novel approach that leverages Large Language Models (LLMs) to automatically switch control modes based on task context. LAMS requires no prior task demonstrations and incrementally improves by integrating user-generated mode-switching examples.</p>              
            </td>
          </tr>

		
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/incre.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://arxiv.org/abs/1503.00848" id="MCG_journal"> -->
              <a href="https://ilsa-robo.github.io/">
<!--               <a> -->
                <papertitle>Incremental Learning for Robot Shared Autonomy</papertitle>
              </a>
              <br>
              <strong>Yiran Tao</strong>, Guixiu Qiao, Dan Ding, Zackory Erickson
              <br>
              <em>Submitted to IROS 2025</em>
              <br>            
              <p>We introduce ILSA, an Incrementally Learned Shared Autonomy framework that continually improves its assistive control policy through repeated user interactions.</p>              
            </td>
          </tr>

		

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/uvad.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://arxiv.org/abs/1503.00848" id="MCG_journal"> -->
              <a href="https://www.sciencedirect.com/science/article/pii/S1047320324001408">
<!--               <a> -->
                <papertitle>Memory-Guided Normality Patterns Representation Matching for Unsupervised Video Anomaly Detection</papertitle>
              </a>
              <br>
              <strong>Yiran Tao</strong>, Yaosi Hu, Zhenzhong Chen
              <br>
              <em>Journal of Visual Communication and Image Representation</em>
              <br>            
              <p>We propose a novel approach for Unsupervised Video Anomaly Detection (UVAD) that directly detects anomalies based on similarities between events in videos. Our method generates representations for events while simultaneously capturing prototypical normality patterns, and detects anomalies based on whether an event‚Äôs representation matches the captured patterns.</p>              
            </td>
          </tr>
	
          
          
         
         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SPL.jpeg" alt="PontTuset" width="160" style="border-style: none"> 
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://arxiv.org/abs/1503.00848" id="MCG_journal"> -->
              <a href="https://ieeexplore.ieee.org/document/9894684">           
                <papertitle>Temporal Weighting Appearance-Aligned Network for Nighttime Video Retrieval</papertitle>
              </a>
              <br>
              Weijian Ruan*, <strong>Yiran Tao*</strong>, Linjun Ruan, Xiujun Shu, Yu Qiao
              <br>
              <em>IEEE Signal Processing Letters</em>
              <br>            
              <p>We build dataset for a novel task, namely video-based person re-identification <em>during nighttime</em>, and propose a temporal weighting appearance-aligned model to tackle this task.</p>              
            </td>
          </tr> 
			
          
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/VCIP.jpeg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://arxiv.org/abs/1503.00848" id="MCG_journal"> -->
              <a href="https://ieeexplore.ieee.org/document/9675397/">
                <papertitle>Learn to Look Around: Deep Reinforcement Learning Agent for Video Saliency Prediction</papertitle>
              </a>
              <br>
              <strong>Yiran Tao</strong>, Yaosi Hu, Zhenzhong Chen
              <br>
              <em>IEEE International Conference on Visual Communications and Image Processing (VCIP)</em>, 2021
              <br>            
              <p>We propose a deep reinforcement learning agent that generates a window of frames containing the most highly correlated information for saliency prediction for each video frame, which assists backbone models to extract temporal information and promotes their performance.</p>
              
            </td>
          </tr>

          
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website is modified from <a href="https://jonbarron.info/">Dr. Jon Barron</a>'s public source code. 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
